{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from itertools import product\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpch_dir = Path(\"../data/tpchv2/\")\n",
    "tpcc_dir = Path(\"../data/tpccv2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### インスタンスが独立に動作していると考え，インスタンスごと(=hostごと)にDataFrameを作成\n",
    "- apjのdfには\"read\", geodataのdfには\"write\"ラベルを貼る\n",
    "- tpchは後で付け足す感じで．(readのラベルを貼る)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lst = []\n",
    "\n",
    "# tpch & tpcc (pseudo data)\n",
    "for data_dir in [tpcc_dir, tpch_dir]:\n",
    "    data_dict = {}\n",
    "    json_paths = data_dir.glob(\"*.json\")\n",
    "    for path in json_paths:\n",
    "        with open(path, \"r\") as f:\n",
    "            metric = path.name[:-24]\n",
    "            results = json.load(f)[\"results\"]\n",
    "            metric_set = set()\n",
    "            for src_data in results:\n",
    "                for data in src_data[\"data\"]:\n",
    "                    metric_set.add(data[\"metric\"])\n",
    "            for src_data in results:\n",
    "                if src_data[\"source\"][:10] != \"summarizer\":\n",
    "                    for data in src_data[\"data\"]:\n",
    "                        # jsonのキーは順番保証されていないので念の為ソート\n",
    "                        tags = [data[\"tags\"][key] for key in sorted(data[\"tags\"].keys()) if key != \"origin\"]\n",
    "                        key_name = metric\n",
    "                        # metricフィールドが全て同じならキー名に含めない\n",
    "                        if len(metric_set) > 1:\n",
    "                            # metricフィールドの冗長な名前を簡潔化\n",
    "                            metric_attr = re.findall('^.*\\.(.*)$', data['metric'])[0]\n",
    "                            key_name += f\"-{metric_attr}\"\n",
    "                        if len(tags) > 0:\n",
    "                            key_name += f\"-{'-'.join(tags)}\"\n",
    "                        data_dict[key_name] = data[\"NumericType\"]\n",
    "    df = pd.DataFrame(data_dict, dtype=\"float32\")\n",
    "    df.dropna(inplace=True)\n",
    "    # query数が0近くから急激に上昇した直後 or 急激に0近くまで下降した直前のデータを取り除く\n",
    "    if data_dir == tpcc_dir:\n",
    "        query_nums = df.filter(like=\"query-\").sum(axis=1)\n",
    "        rm_indices = set(query_nums[query_nums < 500].index.to_list())\n",
    "        rm_indices = rm_indices | {idx + 1 for idx in rm_indices if idx < len(df)} | {idx - 1 for idx in rm_indices if idx > 1} | {1, len(df)}\n",
    "    else:\n",
    "        cpu_usage = df.filter(like=\"cpu-usage_system\").sum(axis=1)\n",
    "        rm_indices = set(cpu_usage[cpu_usage < 8].index.to_list())\n",
    "        rm_indices = rm_indices | {idx + 1 for idx in rm_indices if idx < len(df)} | {idx - 1 for idx in rm_indices if idx > 1} | {1, len(df)}\n",
    "    df.drop(rm_indices, axis=0, inplace=True)\n",
    "    label = \"read\" if data_dir == tpch_dir else \"write\"\n",
    "    df[\"label\"] = [label] * len(df) # ラベルを追加\n",
    "    df_lst.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non duplicated cols:\n",
      "{'query_response_time', 'no_index_used_total'}\n"
     ]
    }
   ],
   "source": [
    "# ワークロード間で重複していないメトリックを探索\n",
    "duplicated_cols = set(df_lst[0].columns)\n",
    "for tmp_df in df_lst:\n",
    "    duplicated_cols &= set(tmp_df.columns)\n",
    "\n",
    "non_duplicated_cols = set()\n",
    "for tmp_df in df_lst:\n",
    "    non_duplicated_cols |= set(tmp_df.columns) ^ duplicated_cols\n",
    "\n",
    "print(\"non duplicated cols:\")\n",
    "print(non_duplicated_cols)\n",
    "\n",
    "# 重複したメトリックを削除\n",
    "if len(non_duplicated_cols) > 0:\n",
    "    for tmp_df in df_lst:\n",
    "        tmp_df.drop(non_duplicated_cols, axis=1, errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 閾値を徐々に下げて削除していく\n",
    "これにより相関が非常に高いものを優先的に削除できる\n",
    "\n",
    "- 特徴量を選択する際は全てのwrite, read intensiveなデータを一緒くたにして相関をみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['handler-write',\n",
      " 'handler-commit',\n",
      " 'cpu-usage_system',\n",
      " 'handler-delete',\n",
      " 'handler-read_rnd_next',\n",
      " 'handler-external_lock',\n",
      " 'handler-read_key',\n",
      " 'handler-read_next',\n",
      " 'handler-update',\n",
      " 'avg_innodb_row_updates-inserted',\n",
      " 'avg_innodb_row_reads-read',\n",
      " 'avg_innodb_row_updates-updated',\n",
      " 'avg_innodb_row_updates-deleted',\n",
      " 'buffer_hit_ratio%',\n",
      " 'disk_used_(%)',\n",
      " 'innodb_io-mysql_global_status_innodb_data_written',\n",
      " 'disk_used_(gb)-used',\n",
      " 'innodb_io-mysql_global_status_innodb_data_read',\n",
      " 'cpu-usage_iowait',\n",
      " 'disk_busy',\n",
      " 'network_io-bytes_sent',\n",
      " 'avg_lock_wait_time',\n",
      " 'cpu-usage_softirq',\n",
      " 'handler-rollback',\n",
      " 'handler-read_first',\n",
      " 'disk_iops-writes',\n",
      " 'handler-read_rnd',\n",
      " 'network_io-bytes_recv',\n",
      " 'cpu-usage_user',\n",
      " 'handler-read_prev',\n",
      " 'avg_mem_used']\n",
      "the number of deleted metrics: 31\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "# df = pd.concat(df_lst[:-1])\n",
    "df = pd.concat(df_lst)\n",
    "df = df.loc[:, df.nunique() != 1] # 値が一定のメトリックを除く\n",
    "df_selected = df.drop([\"label\"], axis=1, inplace=False)\n",
    "\n",
    "labels = df[\"label\"]\n",
    "\n",
    "del_lim = 35\n",
    "del_num = 0\n",
    "ths = [0.95, 0.9, 0.85, 0.8, 0.75, 0.7]\n",
    "del_metrics_all = []\n",
    "\n",
    "for th in ths:\n",
    "    df_corr = df_selected.corr()\n",
    "    corr_mat = df_corr.to_numpy()\n",
    "    cols = df_corr.columns\n",
    "\n",
    "    # 相関が th 以上 or -th 以下のメトリックを取り出す\n",
    "    high_corrs_dict = {k: set() for k in cols}\n",
    "    for i, j in zip(*np.where((corr_mat >= th) | (corr_mat <= -th))):\n",
    "        if i < j:\n",
    "            # queryはworkloadを最もよく表しているので，消さないようにする\n",
    "            if cols[i][:6] != \"query-\":\n",
    "                high_corrs_dict[cols[i]].add(cols[j])\n",
    "            if cols[j][:6] != \"query-\":\n",
    "                high_corrs_dict[cols[j]].add(cols[i])\n",
    "    del_metrics = []\n",
    "    while del_num < del_lim:\n",
    "        # 相関が高いメトリック間の関係数をメトリック別に列挙\n",
    "        # （メトリックごとの関係数を相関係数の和で代用してもいい）\n",
    "        del_metric = max(high_corrs_dict.items(), key=lambda item: len(item[1]))[0]\n",
    "        if len(high_corrs_dict[del_metric]) == 0:\n",
    "            break\n",
    "        # keyを削除\n",
    "        high_corrs_dict.pop(del_metric, None)\n",
    "        # value(=set)の要素を削除\n",
    "        for k, v_set in high_corrs_dict.items():\n",
    "            if del_metric in v_set:\n",
    "                v_set.discard(del_metric)\n",
    "        del_metrics.append(del_metric)\n",
    "        del_num += 1\n",
    "    del_metrics_all += del_metrics\n",
    "    df_selected.drop(del_metrics, axis=1, inplace=True)\n",
    "pprint(del_metrics_all)\n",
    "print(f\"the number of deleted metrics: {del_num}\")\n",
    "\n",
    "# plt.figure(figsize=(25, 25))\n",
    "# sns.heatmap(df_selected.corr(), vmax=1, vmin=-1, center=0, annot=True, square=True, cmap=\"bwr\", annot_kws={\"size\": 16}, fmt=\".2f\", cbar_kws={\"shrink\": 0.85})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, random_state=100)\n",
    "\n",
    "clusters = model.fit(df_selected)\n",
    "\n",
    "# std_scaler = StandardScaler()\n",
    "# df_std = std_scaler.fit_transform(df_selected)\n",
    "# clusters = model.fit(df_std)\n",
    "\n",
    "# mm_scaler = MinMaxScaler()\n",
    "# df_normalized = mm_scaler.fit_transform(df_selected)\n",
    "# clusters = model.fit(df_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, \n",
      "\n",
      "0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, "
     ]
    }
   ],
   "source": [
    "for label_pred in clusters.labels_:\n",
    "    print(label_pred, end=\", \")\n",
    "print('\\n')\n",
    "labels_int = labels.map({\"read\": 1, \"write\": 0}).to_numpy()\n",
    "for label in labels_int:\n",
    "    print(label, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
